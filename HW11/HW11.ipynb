{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist_gui.py\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from PyQt6.QtWidgets import QApplication, QLabel, QVBoxLayout, QWidget, QFileDialog\n",
    "from PyQt6.QtCore import Qt, QSize\n",
    "from PyQt6.QtGui import QPixmap, QDragEnterEvent, QDropEvent\n",
    "\n",
    "class FashionMNISTClassifier(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Class names for Fashion MNIST\n",
    "        self.class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "        \n",
    "        # Load TFLite model\n",
    "        self.interpreter = tf.lite.Interpreter(model_path='fashion_mnist.tflite')\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "        self.initUI()\n",
    "\n",
    "    def initUI(self):\n",
    "        self.setWindowTitle('Fashion MNIST Classifier')\n",
    "        self.setAcceptDrops(True)\n",
    "        self.setMinimumSize(QSize(400, 400))\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "        \n",
    "        # Image display\n",
    "        self.image_label = QLabel(\"Drag & Drop Image Here\")\n",
    "        self.image_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "        self.image_label.setStyleSheet(\"\"\"\n",
    "            QLabel {\n",
    "                border: 2px dashed #aaa;\n",
    "                padding: 20px;\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "        \"\"\")\n",
    "\n",
    "        # Prediction label\n",
    "        self.prediction_label = QLabel(\"Prediction: \")\n",
    "        self.prediction_label.setStyleSheet(\"font-size: 18px; font-weight: bold;\")\n",
    "        self.prediction_label.setAlignment(Qt.AlignmentFlag.AlignCenter)\n",
    "\n",
    "        layout.addWidget(self.image_label)\n",
    "        layout.addWidget(self.prediction_label)\n",
    "        self.setLayout(layout)\n",
    "\n",
    "    def dragEnterEvent(self, event: QDragEnterEvent):\n",
    "        if event.mimeData().hasUrls():\n",
    "            event.accept()\n",
    "        else:\n",
    "            event.ignore()\n",
    "\n",
    "    def dropEvent(self, event: QDropEvent):\n",
    "        file_path = event.mimeData().urls()[0].toLocalFile()\n",
    "        self.process_image(file_path)\n",
    "\n",
    "    def process_image(self, file_path):\n",
    "        try:\n",
    "            # Preprocess image\n",
    "            img = Image.open(file_path).convert('L').resize((28, 28))\n",
    "            img_array = np.array(img).reshape(1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "            # Run inference\n",
    "            self.interpreter.set_tensor(self.input_details[0]['index'], img_array)\n",
    "            self.interpreter.invoke()\n",
    "            predictions = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "            \n",
    "            # Display results\n",
    "            self.image_label.setPixmap(QPixmap(file_path).scaled(200, 200))\n",
    "            predicted_class = self.class_names[np.argmax(predictions)]\n",
    "            confidence = np.max(tf.nn.softmax(predictions))\n",
    "            self.prediction_label.setText(\n",
    "                f\"Prediction: {predicted_class}\\nConfidence: {confidence:.2%}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.prediction_label.setText(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7403 - loss: 0.6975 - val_accuracy: 0.8571 - val_loss: 0.3884\n",
      "Epoch 2/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.8749 - loss: 0.3332 - val_accuracy: 0.8850 - val_loss: 0.3146\n",
      "Epoch 3/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.8953 - loss: 0.2811 - val_accuracy: 0.8882 - val_loss: 0.3044\n",
      "Epoch 4/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9070 - loss: 0.2442 - val_accuracy: 0.9026 - val_loss: 0.2710\n",
      "Epoch 5/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9192 - loss: 0.2173 - val_accuracy: 0.9046 - val_loss: 0.2623\n",
      "Epoch 6/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9269 - loss: 0.1954 - val_accuracy: 0.9075 - val_loss: 0.2596\n",
      "Epoch 7/7\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9351 - loss: 0.1768 - val_accuracy: 0.9103 - val_loss: 0.2621\n",
      "INFO:tensorflow:Assets written to: /var/folders/d5/_b51qwzx4f7__pl7x_l5jnkw0000gn/T/tmptba0uugn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/d5/_b51qwzx4f7__pl7x_l5jnkw0000gn/T/tmptba0uugn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/d5/_b51qwzx4f7__pl7x_l5jnkw0000gn/T/tmptba0uugn'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5407777296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407778448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407776144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407776528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407777872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407777488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407776720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407777104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407779216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5407780368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1739067134.086211 5617156 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739067134.086226 5617156 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-09 03:12:14.086629: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/d5/_b51qwzx4f7__pl7x_l5jnkw0000gn/T/tmptba0uugn\n",
      "2025-02-09 03:12:14.087092: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-09 03:12:14.087100: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/d5/_b51qwzx4f7__pl7x_l5jnkw0000gn/T/tmptba0uugn\n",
      "I0000 00:00:1739067134.091377 5617156 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-02-09 03:12:14.092072: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-09 03:12:14.117235: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/d5/_b51qwzx4f7__pl7x_l5jnkw0000gn/T/tmptba0uugn\n",
      "2025-02-09 03:12:14.125283: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 38656 microseconds.\n",
      "2025-02-09 03:12:14.132279: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "#x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "#x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy,#(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=7, validation_data=(x_test, y_test))\n",
    "\n",
    "# Convert to TFLite with quantization (OPTIONAL)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantization\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save models\n",
    "with open('fashion_mnist.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Save model summary (for slides)\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = QApplication(sys.argv)\n",
    "window = FashionMNISTClassifier()\n",
    "window.resize(400, 400)\n",
    "window.show()\n",
    "sys.exit(app.exec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
